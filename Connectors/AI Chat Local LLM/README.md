### AI Chat with Local LLM

#### Executes a one-off prompt request for each row in the input block, and outputs the results using a local LLM model compatible with the OpenAI API.
#### You can use this block to chat with a local LLM model like *LLamafile AI*. See instructions [here](https://github.com/Mozilla-Ocho/llamafile?tab=readme-ov-file#quickstart),

## Language
Python

## Dependencies
openai

## Source
[script.py](https://github.com/visokio/omniscope-custom-blocks/blob/master/Connectors/AI%20Chat%0Local%20LLM/script.py)
